{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# \ud83d\udd0c Model Context Protocol (MCP): Connecting AI to the World\n",
    "\n",
    "**Scenario:** Imagine you're building an AI assistant for a travel agency. Your assistant needs to check real-time weather, search for flights, look up hotel availability, and calculate currency conversions. Each of these requires a different API or service. Managing all these integrations manually would be a nightmare!\n",
    "\n",
    "This is where **Model Context Protocol (MCP)** comes in. Think of MCP as a universal adapter that lets your AI connect to any tool or service through a standardized interface. Instead of writing custom code for each API, you connect to MCP servers that provide these capabilities.\n",
    "\n",
    "In this notebook, you'll learn how to:\n",
    "- Understand what MCP is and why it matters\n",
    "- Connect to MCP servers\n",
    "- Use MCP tools with any LLM (not just Gemini!)\n",
    "- Build practical MCP-powered applications\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf What You'll Build\n",
    "\n",
    "In this notebook, you will execute external MCP scripts to:\n",
    "1. **Connect to Servers** - Access MCP servers and their tools\n",
    "2. **Manual Tool Calling** - Use MCP with ANY LLM provider\n",
    "3. **Real-world Applications** - Weather bot, data fetcher, multi-tool agent\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Setup: Install Required Packages\n",
    "\n",
    "We'll use:\n",
    "- **`litellm`** - AI model interface (works with any provider)\n",
    "- **`python-dotenv`** - Load API keys\n",
    "- **`mcp`** - Model Context Protocol library\n",
    "- **`langchain-google-genai`** - For native Gemini MCP support (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install_deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q litellm python-dotenv mcp langchain-google-genai google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Using model: openrouter/google/gemini-2.0-flash-001\n",
      "\u2705 Python Executable: /Users/param/learn/learnwithparam/ai-bootcamp-notebooks/notebooks/ai-foundation-workshop/.venv/bin/python3.14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env file (if it exists)\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "DEFAULT_MODEL = os.getenv(\"DEFAULT_MODEL\", \"gemini/gemini-2.0-flash-exp\")\n",
    "\n",
    "print(f\"\u2705 Using model: {DEFAULT_MODEL}\")\n",
    "print(f\"\u2705 Python Executable: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explanation",
   "metadata": {},
   "source": [
    "## \ud83e\udd14 Part 1: Understanding MCP\n",
    "\n",
    "**What is MCP?** Model Context Protocol is like a universal remote control for AI. Instead of your AI needing to know how to talk to each specific service (weather API, database, file system), it talks to MCP servers that provide standardized access to these services.\n",
    "\n",
    "**Why does this matter?**\n",
    "- **Standardization**: One way to connect to many services\n",
    "- **Flexibility**: Works with ANY LLM, not just one provider\n",
    "- **Simplicity**: MCP servers handle the complex API details\n",
    "- **Reusability**: Same MCP server works with different AI applications\n",
    "\n",
    "**Real-World Analogy:** Think of MCP like USB ports. Before USB, every device had its own connector. Now, everything uses USB. MCP does the same for AI tools!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udf24\ufe0f Part 2: Connecting to an MCP Server (Weather Example)\n",
    "\n",
    "We will run the `mcp_weather.py` script. This script connects to the `@philschmid/weather-mcp` server via `npx` and lists the available tools.\n",
    "\n",
    "**Note:** We run this as a subprocess to ensure a stable `asyncio` event loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "write_mcp_weather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_weather.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_weather.py\n",
    "import asyncio\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import json\n",
    "\n",
    "# Define the MCP server configuration\n",
    "# We use 'npx' to run the weather server directly from npm\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"npx\",\n",
    "    args=[\"-y\", \"@philschmid/weather-mcp\"],\n",
    "    env=None\n",
    ")\n",
    "\n",
    "async def explore_mcp_server():\n",
    "    print(\"Connecting to weather server... (this may take a moment to download the npx package)\")\n",
    "    \n",
    "    # Establish the connection via Stdio (Standard Input/Output)\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the MCP session\n",
    "            await session.initialize()\n",
    "            print(f\"\u2713 Connected to MCP weather server!\\n\")\n",
    "            \n",
    "            # List available tools provided by the server\n",
    "            tools_result = await session.list_tools()\n",
    "            tools = tools_result.tools\n",
    "            \n",
    "            print(f\"\u2192 Available Tools:\")\n",
    "            for tool in tools:\n",
    "                print(f\"  \u2022 {tool.name}: {tool.description}\")\n",
    "                print(f\"    Parameters: {json.dumps(tool.inputSchema, indent=6)}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(explore_mcp_server())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "run_mcp_weather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running mcp_weather.py using /Users/param/learn/learnwithparam/ai-bootcamp-notebooks/notebooks/ai-foundation-workshop/.venv/bin/python3.14...\n",
      "Connecting to weather server... (this may take a moment to download the npx package)\n",
      "\u2713 Connected to MCP weather server!\n",
      "\n",
      "\u2192 Available Tools:\n",
      "  \u2022 get_weather_forecast: Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour.\n",
      "    Parameters: {\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "            \"location\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
      "            },\n",
      "            \"date\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"description\": \"the forecasting date for when to get the weather format (yyyy-mm-dd)\"\n",
      "            }\n",
      "      },\n",
      "      \"required\": [\n",
      "            \"location\",\n",
      "            \"date\"\n",
      "      ],\n",
      "      \"additionalProperties\": false,\n",
      "      \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Running mcp_weather.py using {sys.executable}...\")\n",
    "!{sys.executable} mcp_weather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3_header",
   "metadata": {},
   "source": [
    "### \u2753 Discussion\n",
    "\n",
    "The output above shows the tools available on the server (like `get_weather_forecast`).\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udee0\ufe0f Part 3: Manual Tool Calling (Works with ANY LLM)\n",
    "\n",
    "We will now create an agent script that:\n",
    "1. Connects to the MCP server.\n",
    "2. **Injects the current date** into the prompt (Context Grounding).\n",
    "3. Asks the LLM to pick a tool.\n",
    "4. Executes the tool and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "write_mcp_agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_agent.py\n",
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from litellm import completion\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DEFAULT_MODEL = os.getenv(\"DEFAULT_MODEL\", \"gemini/gemini-2.0-flash-exp\")\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"npx\",\n",
    "    args=[\"-y\", \"@philschmid/weather-mcp\"],\n",
    "    env=None\n",
    ")\n",
    "\n",
    "async def run_agent():\n",
    "    print(\"Starting MCP Agent Workflow...\")\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            \n",
    "            # 1. Get Tools\n",
    "            tools_result = await session.list_tools()\n",
    "            tools = tools_result.tools\n",
    "            tools_desc = \"\\n\".join([f\"- {t.name}: {t.description} (Params: {json.dumps(t.inputSchema)})\" for t in tools])\n",
    "            \n",
    "            # 2. Context Grounding: Get Current Date\n",
    "            current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "            print(f\"[System] Current Time: {current_time}\")\n",
    "            \n",
    "            # 3. Ask AI to pick a tool\n",
    "            question = \"What is the weather in Berlin today?\"\n",
    "            print(f\"\\nUser Question: {question}\")\n",
    "            \n",
    "            prompt = f\"\"\"System: You are a helpful assistant. The current date and time is {current_time}.\n",
    "            \n",
    "Available tools:\n",
    "{tools_desc}\n",
    "\n",
    "User question: {question}\n",
    "\n",
    "Respond ONLY with JSON: {{\"tool\": \"tool_name\", \"parameters\": {{...}}}}\"\"\"\n",
    "            \n",
    "            response = completion(model=DEFAULT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "            content = response.choices[0].message.content\n",
    "            \n",
    "            try:\n",
    "                # Clean up potential markdown code blocks\n",
    "                clean_content = content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                action = json.loads(clean_content)\n",
    "                \n",
    "                tool_name = action[\"tool\"]\n",
    "                tool_args = action[\"parameters\"]\n",
    "                print(f\"\\nAI Decided to call: {tool_name} with {tool_args}\")\n",
    "                \n",
    "                # 4. Call the tool via MCP\n",
    "                result = await session.call_tool(tool_name, tool_args)\n",
    "                tool_output = result.content[0].text\n",
    "                print(f\"Tool Output: {tool_output}\")\n",
    "                \n",
    "                # 5. Final Answer\n",
    "                final_prompt = f\"Question: {question}\\nData: {tool_output}\\nProvide a natural answer.\"\n",
    "                final_res = completion(model=DEFAULT_MODEL, messages=[{\"role\": \"user\", \"content\": final_prompt}])\n",
    "                print(f\"\\nFINAL ANSWER: {final_res.choices[0].message.content}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing or executing: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_agent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "run_agent_script",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running mcp_agent.py using /Users/param/learn/learnwithparam/ai-bootcamp-notebooks/notebooks/ai-foundation-workshop/.venv/bin/python3.14...\n",
      "Starting MCP Agent Workflow...\n",
      "[System] Current Time: 2025-11-25 23:06\n",
      "\n",
      "User Question: What is the weather in Berlin today?\n",
      "\n",
      "AI Decided to call: get_weather_forecast with {'location': 'Berlin, Germany', 'date': '2025-11-25'}\n",
      "Tool Output: {\"2025-11-25T00:00\":1.3,\"2025-11-25T01:00\":1.3,\"2025-11-25T02:00\":1.6,\"2025-11-25T03:00\":1.6,\"2025-11-25T04:00\":1.7,\"2025-11-25T05:00\":1.5,\"2025-11-25T06:00\":1.6,\"2025-11-25T07:00\":1.5,\"2025-11-25T08:00\":1.6,\"2025-11-25T09:00\":1.9,\"2025-11-25T10:00\":2.4,\"2025-11-25T11:00\":3.4,\"2025-11-25T12:00\":4.2,\"2025-11-25T13:00\":4.7,\"2025-11-25T14:00\":4.9,\"2025-11-25T15:00\":4.5,\"2025-11-25T16:00\":3.9,\"2025-11-25T17:00\":2.9,\"2025-11-25T18:00\":2.9,\"2025-11-25T19:00\":2.6,\"2025-11-25T20:00\":1.9,\"2025-11-25T21:00\":1.3,\"2025-11-25T22:00\":0.7,\"2025-11-25T23:00\":0.1}\n",
      "\n",
      "FINAL ANSWER: The temperature in Berlin today, November 25, 2025, ranged from a low of 0.1 degrees Celsius to a high of 4.9 degrees Celsius.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the agent script\n",
    "print(f\"Running mcp_agent.py using {sys.executable}...\")\n",
    "!{sys.executable} mcp_agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outro",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Next Steps:\n",
    "\n",
    "- **Build** real API integrations (weather, search, databases)\n",
    "- **Add** authentication and rate limiting\n",
    "- **Combine** with structured outputs for better reliability\n",
    "- **Create** multi-agent systems with specialized tools\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda Additional Resources\n",
    "\n",
    "- [Model Context Protocol (MCP) Documentation](https://modelcontextprotocol.io/)\n",
    "- [MCP Servers on npm](https://www.npmjs.com/search?q=mcp)\n",
    "- [Building MCP Servers](https://modelcontextprotocol.io/docs/develop/build-server)\n",
    "\n",
    "Happy building! \ud83c\udf89\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}